{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca2686-757e-4410-9ea0-7dfd3d4f745a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate bitsandbytes sagemaker openai langchain milvus openai xformers pymilvus chromadb==0.5.3 pydantic==1.10.8 sentence_transformers tiktoken fitz frontend tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb902f5-6099-4ec4-94a9-ea730419acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch\n",
    "import fitz\n",
    "# import boto3\n",
    "# import sagemaker\n",
    "import re\n",
    "import chromadb\n",
    "# from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "from pymilvus import FieldSchema, MilvusClient,CollectionSchema, DataType, utility,Collection\n",
    "import tiktoken\n",
    "from transformers import  AutoTokenizer,AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab70d15-3812-47e0-a389-ccd185b2bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_comments_from_the_pdf(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    posts_pattern = r\"Title of the post:(.*?)(?=Title of the post:|\\Z)\"\n",
    "    posts = re.findall(posts_pattern, text, re.DOTALL)\n",
    "    posts_dict = {}\n",
    "    for post_value in tqdm(posts):\n",
    "        title_to_body_pattern = r\"(.*?)(?=Comment of the post: |\\Z)\"\n",
    "        title_to_body = re.findall(title_to_body_pattern, post_value, re.DOTALL)\n",
    "        try:\n",
    "            title_to_body = title_to_body[0]\n",
    "        except Exception as e:\n",
    "            import pdb;pdb.set_trace()\n",
    "        list_of_comments = []\n",
    "        comments_to_title_pattern = r\"Comment of the post:(.*?)(?=Title of the post:|\\Z)\"\n",
    "        comments = re.findall(comments_to_title_pattern, post_value, re.DOTALL)\n",
    "        if len(comments) > 0:\n",
    "            individual_comments_pattern = r\"New Comment: (.*?)(?=New Comment: |\\Z)\"\n",
    "            individual_comments = re.findall(individual_comments_pattern,comments[0], re.DOTALL)\n",
    "        list_of_comments = [i for i in individual_comments if len(i) > 0]\n",
    "        if title_to_body in posts_dict.keys():\n",
    "            import pdb;pdb.set_trace()\n",
    "        posts_dict[title_to_body] = list_of_comments\n",
    "    return posts_dict\n",
    "with open(\"post_comment_mapping_final.json\",\"r\") as f:\n",
    "    post_comment_mapping  =json.load(f)\n",
    "post_id_to_posts_text_mapping = {}\n",
    "post_ids_to_comment_ids_mapping = {}\n",
    "comments_ids_to_comment_text_mappings = {}\n",
    "post_counter = 0\n",
    "comment_counter = 0\n",
    "for item,value in post_comment_mapping.items():\n",
    "    post_id_to_posts_text_mapping[post_counter] = item\n",
    "    comment_ids_list = []\n",
    "    for comment_value in value:\n",
    "        comments_ids_to_comment_text_mappings[comment_counter] = comment_value\n",
    "        comment_ids_list.append(comment_counter)\n",
    "        comment_counter = comment_counter + 1\n",
    "    post_ids_to_comment_ids_mapping[post_counter] = comment_ids_list\n",
    "    post_counter = post_counter + 1\n",
    "with open(\"post_id_to_posts_text_mapping.json\", \"w\") as f:\n",
    "    json.dump(post_id_to_posts_text_mapping,f)\n",
    "with open(\"post_ids_to_comment_ids_mapping.json\", \"w\") as f:\n",
    "    json.dump(post_ids_to_comment_ids_mapping,f)\n",
    "with open(\"comments_ids_to_comment_text_mappings.json\", \"w\") as f:\n",
    "    json.dump(comments_ids_to_comment_text_mappings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d84cc80-1e3f-427a-b672-4927eb0f6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data_using_open_ai_model(tokenizer,text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) > 8191:\n",
    "        tokens = tokens[:8191]\n",
    "    truncated_text = tokenizer.decode(tokens)\n",
    "    # openai.api_key =\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=truncated_text,\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "def embedding_main(embedding_model_type,text,query_embedding_bool):\n",
    "    if embedding_model_type!=\"gpt\":\n",
    "        vector_dim = 1024\n",
    "        vector_linear_directory = f\"2_Dense_{vector_dim}\"\n",
    "        model_dir = \"dunzhang/stella_en_400M_v5\"\n",
    "        model = AutoModel.from_pretrained(model_dir, trust_remote_code=True,use_memory_efficient_attention=False,unpad_inputs=False).cpu().eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        vector_linear = torch.nn.Linear(in_features=model.config.hidden_size, out_features=vector_dim)\n",
    "        vector_linear_dict = {\n",
    "            k.replace(\"linear.\", \"\"): v for k, v in\n",
    "            torch.load(\"pytorch_model.bin\",map_location=torch.device('cpu')).items()\n",
    "        }\n",
    "        vector_linear.load_state_dict(vector_linear_dict)\n",
    "        vector_linear.cpu()\n",
    "        embedding = embed_data_using_stella_model(tokenizer,model,text,query_embedding_bool,vector_linear)\n",
    "    else:\n",
    "        model_name='text-embedding-ada-002'\n",
    "        tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        embedding = embed_data_using_open_ai_model(tokenizer,text)\n",
    "        # embedding_size = 1536\n",
    "        # print (embedding.shape)\n",
    "    return embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7845fda9-eee6-4194-aa0e-4d61042031eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_into_vector_store_chromadb(embedding_method,post_embeddings_list,post_ids_list,comment_embeddings_list,comment_ids_list):\n",
    "    chroma_client = chromadb.PersistentClient(f'./chroma_sagemaker_{str(embedding_method)}')\n",
    "    existing_collections = chroma_client.list_collections()\n",
    "    names_collections = [i.name for i in existing_collections]\n",
    "    if (\"posts\" in names_collections):\n",
    "        posts_collection = chroma_client.get_collection(name=\"posts\")\n",
    "        chroma_client.delete_collection(name='posts')\n",
    "    if (\"comments\" in names_collections):\n",
    "        comments_collection = chroma_client.get_collection(name=\"comments\")\n",
    "        chroma_client.delete_collection(name='comments')\n",
    "    posts_collection = chroma_client.create_collection(name=\"posts\")\n",
    "    comments_collection = chroma_client.create_collection(name=\"comments\")\n",
    "    posts_collection.add(embeddings=post_embeddings_list, ids=post_ids_list)\n",
    "    comments_collection.add(embeddings=comment_embeddings_list, ids=comment_ids_list)\n",
    "    # data_to_export = {\n",
    "    # \"post_ids\": post_ids_list,\n",
    "    # \"post_embeddings\": post_embeddings_list,\n",
    "    # \"comment_ids\": comment_ids_list,\n",
    "    # \"comment_embeddings\": comment_embeddings_list\n",
    "    # }\n",
    "    # with open(f'{embedding_method}_chromadb_embeddings_export.json', 'w') as f:\n",
    "    #     json.dump(data_to_export, f)\n",
    "\n",
    "def store_data_into_vector_store_milvus(embedding_type,post_embeddings_list,post_ids_list,comment_embeddings_list,comment_ids_list,VECTOR_INDEX_METHOD,EMBEDDING_SIZE):\n",
    "    # connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "    if embedding_type != \"gpt\":\n",
    "        embedding_type = \"stella\"\n",
    "    client = MilvusClient(f\"./{embedding_type}_milvus_demo_{VECTOR_INDEX_METHOD}.db\")\n",
    "    posts_schema = client.create_schema(auto_id=False,enable_dynamic_field=False)\n",
    "    posts_schema.add_field(field_name=\"post_id_field\", datatype=DataType.INT64, is_primary=True)\n",
    "    posts_schema.add_field(field_name=\"post_vector_field\", datatype=DataType.FLOAT_VECTOR, dim=EMBEDDING_SIZE)\n",
    "    comments_schema = client.create_schema(auto_id=False,enable_dynamic_field=False)\n",
    "    comments_schema.add_field(field_name=\"comment_id_field\", datatype=DataType.INT64, is_primary=True)\n",
    "    comments_schema.add_field(field_name=\"comment_vector_field\", datatype=DataType.FLOAT_VECTOR, dim=EMBEDDING_SIZE)\n",
    "    post_collection = client.create_collection(collection_name=\"posts_collection\", schema=posts_schema)\n",
    "    comment_collection = client.create_collection(collection_name=\"comments_collection\", schema=comments_schema)\n",
    "    posts_index_params = client.prepare_index_params(collection_name=\"posts_collection\")\n",
    "    comments_index_params = client.prepare_index_params(collection_name=\"comments_collection\")\n",
    "    posts_index_params.add_index(field_name='post_vector_field', metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"M\": 16, \"efConstruction\": 200})\n",
    "    if VECTOR_INDEX_METHOD == \"HNSW\":\n",
    "        # index_params = {\"metric_type\": \"COSINE\", \"index_type\": VECTOR_INDEX_METHOD, \"params\": {\"M\": 16, \"efConstruction\": 200}}\n",
    "        posts_index_params.add_index(field_name='post_vector_field', metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"M\": 16, \"efConstruction\": 200})\n",
    "        comments_index_params.add_index(field_name='comment_vector_field', metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"M\": 16, \"efConstruction\": 200})\n",
    "    elif VECTOR_INDEX_METHOD == \"IVF_FLAT\":\n",
    "        # index_params = {\"metric_type\": \"COSINE\", \"index_type\": VECTOR_INDEX_METHOD, \"params\": {\"nlist\": 128}}\n",
    "        posts_index_params.add_index(field_name='post_vector_field',metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"nlist\": 128})\n",
    "        comments_index_params.add_index(field_name='comment_vector_field',metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"nlist\": 128})\n",
    "    elif VECTOR_INDEX_METHOD == \"ANNOY\":\n",
    "        # index_params = {\"metric_type\": \"COSINE\", \"index_type\": VECTOR_INDEX_METHOD, \"params\": {\"n_trees\": 50}}\n",
    "        posts_index_params.add_index(field_name='post_vector_field',metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"n_trees\": 50})\n",
    "        comments_index_params.add_index(field_name='comment_vector_field',metric_type=\"COSINE\",index_type=str(VECTOR_INDEX_METHOD),params={\"n_trees\": 50})\n",
    "    elif VECTOR_INDEX_METHOD ==\"BIN_FLAT\":\n",
    "        posts_index_params.add_index(field_name='post_vector_field', metric_type=\"COSINE\", index_type=str(VECTOR_INDEX_METHOD), params={})\n",
    "        comments_index_params.add_index(field_name='comment_vector_field', metric_type=\"COSINE\", index_type=str(VECTOR_INDEX_METHOD), params={})\n",
    "    elif VECTOR_INDEX_METHOD ==\"FLAT\":\n",
    "        posts_index_params.add_index(field_name='post_vector_field', metric_type=\"COSINE\", index_type=str(VECTOR_INDEX_METHOD), params={})\n",
    "        comments_index_params.add_index(field_name='comment_vector_field', metric_type=\"COSINE\", index_type=str(VECTOR_INDEX_METHOD), params={})\n",
    "    else:\n",
    "        import pdb;pdb.set_trace()\n",
    "    client.create_index(collection_name=\"posts_collection\", index_params=posts_index_params)\n",
    "    client.create_index(collection_name=\"comments_collection\", index_params=comments_index_params)\n",
    "    client.load_collection(collection_name=\"posts_collection\")\n",
    "    client.load_collection(collection_name=\"comments_collection\")\n",
    "    print (len(post_ids_list))\n",
    "    print (len(post_embeddings_list))\n",
    "    post_ids_list = [int(i) for i in post_ids_list]\n",
    "    comment_ids_list = [int(i) for i in comment_ids_list]\n",
    "\n",
    "    print (type(post_ids_list[0]))\n",
    "    print (type(post_embeddings_list[0]))\n",
    "    # Check if each embedding is correctly formatted\n",
    "    for embedding in post_embeddings_list:\n",
    "        if len(embedding) != EMBEDDING_SIZE:\n",
    "            print(f\"Embedding of length {len(embedding)} found, expected {EMBEDDING_SIZE}\")\n",
    "            raise ValueError(\"Incorrect embedding size\")\n",
    "\n",
    "    # Ensure all elements are floats\n",
    "    for embedding in post_embeddings_list:\n",
    "        if not all(isinstance(x, float) for x in embedding):\n",
    "            raise ValueError(\"Embedding contains non-float values\")\n",
    "    data_posts_insertion = []\n",
    "    data_comments_insertion = []\n",
    "    for index_value in range(len(post_ids_list)):\n",
    "        internal_dict = {}\n",
    "        internal_dict['post_id_field'] = post_ids_list[index_value]\n",
    "        internal_dict['post_vector_field'] = post_embeddings_list[index_value]\n",
    "        data_posts_insertion.append(internal_dict)\n",
    "    for index_value in range(len(comment_ids_list)):\n",
    "        internal_dict = {}\n",
    "        internal_dict['comment_id_field'] = comment_ids_list[index_value]\n",
    "        internal_dict['comment_vector_field'] = comment_embeddings_list[index_value]\n",
    "        data_comments_insertion.append(internal_dict)\n",
    "    client.insert(\"posts_collection\",data=data_posts_insertion)\n",
    "    client.insert(\"comments_collection\",data=data_comments_insertion)\n",
    "\n",
    "    data_to_export = {\n",
    "    \"post_ids\": post_ids_list,\n",
    "    \"post_embeddings\": post_embeddings_list,\n",
    "    \"comment_ids\": comment_ids_list,\n",
    "    \"comment_embeddings\": comment_embeddings_list\n",
    "    }\n",
    "    with open(f'{embedding_type}_milvus_embeddings_export_{VECTOR_INDEX_METHOD}.json', 'w') as f:\n",
    "        json.dump(data_to_export, f)\n",
    "\n",
    "def store_data_into_postgres():\n",
    "    pass\n",
    "def vector_store_main(embedding_method,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,comment_ids_list,vector_store,\n",
    "                      vector_index_method):\n",
    "    if vector_store==\"chromadb\":\n",
    "        store_data_into_vector_store_chromadb(embedding_method,post_embeddings_list,post_ids_list,comment_embeddings_list,comment_ids_list)\n",
    "    elif vector_store==\"postgres\":\n",
    "        store_data_into_postgres()\n",
    "    elif vector_store==\"milvus\":\n",
    "        store_data_into_vector_store_milvus(embedding_method,post_embeddings_list,post_ids_list,comment_embeddings_list,comment_ids_list,vector_index_method,\n",
    "                                            embedding_size)\n",
    "    else:\n",
    "        import pdb;pdb.set_trace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154fd481",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n",
    "number_gpus = 1\n",
    "max_model_len = 8192\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0db8ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/dbt_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/sentence-t5-large')\n",
    "x = embed_data_using_stella_model(model,\"hello how are you\")\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0132c1e6-dfc6-4f4d-a3c1-d2891fa659c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data_using_stella_model(model,text):\n",
    "    embeddings = model.encode(text)\n",
    "    return embeddings\n",
    "    # query_vectors = None\n",
    "    # if query_embedding_bool:\n",
    "    #     with torch.no_grad():\n",
    "    #         input_data = tokenizer(text, padding=\"longest\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "    #         input_data = {k: v.cpu() for k, v in input_data.items()}\n",
    "    #         attention_mask = input_data[\"attention_mask\"]\n",
    "    #         last_hidden_state = model(**input_data)[0]\n",
    "    #         last_hidden = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    #         query_vectors = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    #         query_vectors = normalize(vector_linear(query_vectors).cpu().numpy())\n",
    "    #     if query_vectors is None:\n",
    "    #         import pdb;pdb.set_trace()\n",
    "    #     return query_vectors\n",
    "    # else:\n",
    "    #     docs_vectors = None\n",
    "    #     with torch.no_grad():\n",
    "    #         # print (len(text))\n",
    "    #         # print (\"docs vector\")\n",
    "    #         input_data = tokenizer(text, padding=\"longest\", truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "    #         input_data = {k: v.cpu() for k, v in input_data.items()}\n",
    "    #         attention_mask = input_data[\"attention_mask\"]\n",
    "    #         # print (input_data)\n",
    "    #         last_hidden_state = model(**input_data)[0]\n",
    "    #         last_hidden = last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    #         docs_vectors = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    #         docs_vectors = normalize(vector_linear(docs_vectors).cpu().numpy())\n",
    "    #         # import pdb;pdb.set_trace()\n",
    "    #     # if docs_vectors is None:\n",
    "    #     #     import pdb;pdb.set_trace()\n",
    "    #     return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a060ac0-c537-4b68-b911-91186015e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et stella_en_400M_v5_gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10792/2458559363.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"pytorch_model.bin\",map_location=torch.device('cpu')).items()\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_10792/2458559363.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\"pytorch_model.bin\",map_location=torch.device('cpu')).items()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "<class 'int'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Embedding contains non-float values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m comment_ids_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m comment_ids_list]\n\u001b[1;32m     38\u001b[0m vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\n\u001b[1;32m     39\u001b[0m               comment_ids_list,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchromadb\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mvector_store_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43met\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpost_embeddings_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpost_ids_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcomment_embeddings_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcomment_ids_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmilvus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFLAT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\n\u001b[1;32m     43\u001b[0m               comment_ids_list,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilvus\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHNSW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#               comment_ids_list,'milvus',\"BIN_FLAT\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 114\u001b[0m, in \u001b[0;36mvector_store_main\u001b[0;34m(embedding_method, embedding_size, post_embeddings_list, post_ids_list, comment_embeddings_list, comment_ids_list, vector_store, vector_index_method)\u001b[0m\n\u001b[1;32m    112\u001b[0m     store_data_into_postgres()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vector_store\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilvus\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mstore_data_into_vector_store_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpost_embeddings_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpost_ids_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcomment_embeddings_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcomment_ids_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvector_index_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m;pdb\u001b[38;5;241m.\u001b[39mset_trace()\n",
      "Cell \u001b[0;32mIn[19], line 80\u001b[0m, in \u001b[0;36mstore_data_into_vector_store_milvus\u001b[0;34m(embedding_type, post_embeddings_list, post_ids_list, comment_embeddings_list, comment_ids_list, VECTOR_INDEX_METHOD, EMBEDDING_SIZE)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m post_embeddings_list:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m embedding):\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding contains non-float values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m data_posts_insertion \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     82\u001b[0m data_comments_insertion \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Embedding contains non-float values"
     ]
    }
   ],
   "source": [
    "post_id_to_posts_text_mapping = {k: post_id_to_posts_text_mapping[k] for k in list(post_id_to_posts_text_mapping)[:10]}\n",
    "comments_ids_to_comment_text_mappings = {k: comments_ids_to_comment_text_mappings[k] for k in list(comments_ids_to_comment_text_mappings)[:10]}\n",
    "embedding_types = [\"sentence-t5-large'\"]\n",
    "# stella_en_400M_v5_gguf\n",
    "vector_stores = ['chromadb','milvus']\n",
    "for et in embedding_types:\n",
    "    print (\"et\",et)\n",
    "    post_id_post_embeddings_mapping = {}\n",
    "    comment_id_comment_embeddings_mapping = {}\n",
    "    for item,value in post_id_to_posts_text_mapping.items():\n",
    "        post_embedding = embedding_main(et,value,False)\n",
    "        post_id_post_embeddings_mapping[item] = post_embedding\n",
    "    for item,value in comments_ids_to_comment_text_mappings.items():\n",
    "        comment_embedding = embedding_main(et,value,False)\n",
    "        comment_id_comment_embeddings_mapping[item] = comment_embedding\n",
    "    post_embeddings_list = []\n",
    "    post_embeddings_list = post_embeddings_list +  [v for k,v in post_id_post_embeddings_mapping.items()]\n",
    "    post_ids_list = list(post_id_post_embeddings_mapping.keys())\n",
    "    post_ids_list = [str(i) for i in post_ids_list]\n",
    "    comment_embeddings_list = []\n",
    "    comment_embeddings_list = comment_embeddings_list + [v for k,v in comment_id_comment_embeddings_mapping.items()]\n",
    "    comment_ids_list = list(comment_id_comment_embeddings_mapping.keys())\n",
    "    comment_ids_list = [str(i) for i in comment_ids_list]\n",
    "\n",
    "    if et == \"gpt\":\n",
    "        embedding_size = 1536\n",
    "    else:\n",
    "        embedding_size = 1024\n",
    "    post_ids_list = [str(i) for i in post_ids_list]\n",
    "    comment_ids_list = [str(i) for i in comment_ids_list]\n",
    "    vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\n",
    "                  comment_ids_list,'chromadb',None)\n",
    "    vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\n",
    "                  comment_ids_list,'milvus',\"FLAT\")\n",
    "    vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\n",
    "                  comment_ids_list,'milvus',\"HNSW\")\n",
    "    # vector_store_main(et,embedding_size,post_embeddings_list,post_ids_list,comment_embeddings_list,\n",
    "    #               comment_ids_list,'milvus',\"BIN_FLAT\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "604499b8-56bb-473d-b07b-c77aa7798dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "for out_index in range(len(post_embeddings_list)):\n",
    "    for in_index in range(len(post_embeddings_list)):\n",
    "        if isinstance((post_embeddings_list[out_index][in_index]),numpy.float32):\n",
    "            print (type(post_embeddings_list[out_index][in_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b6201-60a6-4842-a89e-841310fc7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!set COMMANDLINE_ARGS=--xformers --reinstall-xformers\n",
    "set XFORMERS_PACKAGE=xformers==0.0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a71ff-432f-4f12-8eac-5a498590dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5b139-463a-43ea-ad7e-dcab45ae31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir my_model_directory\n",
    "# !curl -o my_model_directory/config.json https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/config.json\n",
    "# !curl -o my_model_directory/pytorch_model.bin https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/pytorch_model.bin\n",
    "# !curl -o my_model_directory/tokenizer.json https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/tokenizer.json\n",
    "# !curl -o my_model_directory/special_tokens_map.json https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/special_tokens_map.json\n",
    "# !curl -o my_model_directory/model.safetensors https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/model.safetensors\n",
    "# !curl -o my_model_directory/vocab.txt https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/vocab.txt\n",
    "# # curl -o my_model_directory/vocab.txt https://huggingface.co/anuna-mbrown/stella_en_400M_v5_gguf/tree/main/stella_en_400M_v5.gguf/main/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9123c61-60ee-484d-979f-42c4af05f8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
